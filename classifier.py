# -*- coding: utf-8 -*-
"""Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RPfYLpkMplf-u8VTcBiRSfoOPfeHVY_A
"""

# -*- coding: utf-8 -*-
"""Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fCb2Qkf1tUir1w4MpMu44DC3Vqmwmt9
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Generative and Discriminative Classifier
"""
import sys
import re
import nltk
import numpy as np
import torch
from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline, padded_everygrams
from nltk.lm import Laplace
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import Dataset, DatasetDict, load_metric
import random

nltk.download('punkt')
nltk.download('stopwords')

# Generative Classifier Classes (Dev and Test) remain unchanged

class Dev:
    def __init__(self, authorlist):
        self.authorlist = authorlist.split("\n")

    def split_data(self, text):
        train_data = []
        dev_data = []
        index = 0
        while index < len(text):
            if index % 5 == 0:
                dev_data.append(text[index])
            else:
                train_data.append(text[index])
            index += 1
        return (train_data, dev_data)

    def edit_text_dev(self, text):
        dev = []
        for sentence in text:
            new_sentence = re.sub("[\‘\”\“\’\'\"]", "", sentence)
            new_sentence = nltk.word_tokenize(new_sentence)
            new_sentence = list(padded_everygrams(2, new_sentence))
            dev.append(new_sentence)
        return dev

    def edit_text_train(self, text):
        new_text = []
        for sentence in text:
            new_sentence = re.sub("[\‘\”\“\’\'\"]", "", sentence)
            new_sentence = nltk.word_tokenize(new_sentence)
            new_text.append(new_sentence)
        train, vocab = padded_everygram_pipeline(2, new_text)
        return (train, vocab)

    def test(self, models, dev_sets):
        for i in range(len(dev_sets)):
            count_correct_author = 0
            correct_author = models[i][0]
            for sentence in dev_sets[i]:
                new_sentence = list(sentence)
                author = None
                perplexity = float('inf')
                for model in models:
                    p = model[1].perplexity(new_sentence)
                    if p < perplexity:
                        perplexity = p
                        author = model[0]
                if author == correct_author:
                    count_correct_author +=1
            accuracy = count_correct_author / len(dev_sets[i]) * 100
            print(f"{correct_author}\t{accuracy:.2f}% correct\n")
            print(f"{correct_author} perplexity: {p}\n")


    def generate_text_samples(self, models):
        print("\nGenerated text samples:")
        for author, model in models:
            print(f"\nGenerating text from {author}'s model:")
            for _ in range(5):
                generated_text = ' '.join(model.generate(20, random_seed=42))
                print(f"Sample: {generated_text}")

    def run(self):
        models = []
        dev_sets = []
        for author in self.authorlist:
            name = author.replace(".txt", "").replace("_utf8", "")
            text = open(author, encoding="utf8").read().lower()
            text = nltk.sent_tokenize(text)
            train_data, dev_data = self.split_data(text)
            train, vocab = self.edit_text_train(train_data)
            dev_data = self.edit_text_dev(dev_data)
            model = Laplace(2)
            model.fit(train, vocab)
            models.append((name, model))
            dev_sets.append(dev_data)
        self.test(models, dev_sets)
        self.generate_text_samples(models)

class Test:
    def __init__(self, authorlist, test):
        self.authorlist = authorlist.split("\n")
        self.testfile = test

    def edit_text_test(self, text):
        data = []
        text = nltk.sent_tokenize(text)
        for sentence in text:
            new_sentence = re.sub("[\‘\”\“\’\'\"]", "", sentence)
            new_sentence = nltk.word_tokenize(new_sentence)
            new_sentence = list(padded_everygrams(2, new_sentence))
            data.append(new_sentence)
        return data

    def edit_text_train(self, text):
        new_text = []
        text = nltk.sent_tokenize(text)
        for sentence in text:
            new_sentence = re.sub("[\‘\”\“\’\'\"]", "", sentence)
            new_sentence = nltk.word_tokenize(new_sentence)
            new_text.append(new_sentence)
        data, vocab = padded_everygram_pipeline(2, new_text)
        return (data, vocab)

    def test(self, models, test_data):
        for sentence in test_data:
            new_sentence = list(sentence)
            author = None
            perplexity = float('inf')
            for model in models:
                p = model[1].perplexity(new_sentence)
                if p < perplexity:
                    perplexity = p
                    author = model[0]
            print(author)

    def run(self):
        models = []
        for author in self.authorlist:
            name = author.replace(".txt", "").replace("_utf8", "")
            text = open(author, encoding="utf8").read().lower()
            train, vocab = self.edit_text_train(text)
            model = Laplace(2)
            model.fit(train, vocab)
            models.append((name, model))
        test_data = self.edit_text_test(self.testfile.lower())
        self.test(models, test_data)

tokenizer = None

def create_dataset(author_dict):
    full_list_text = []
    full_list_label = []
    for file_path, author in author_dict.items():
        with open(file_path, 'r') as file:
            lines = file.readlines()
            text_data = ''.join(lines)
        text_without_newlines = text_data.replace('\n', ' ')
        sentences = sent_tokenize(text_without_newlines)
        full_list_text += sentences
        full_list_label += [author for _ in range(len(sentences))]
    combined = list(zip(full_list_text, full_list_label))
    random.shuffle(combined)
    text_data, author_data = zip(*combined)
    total_length = len(text_data)
    train_length = int(0.8 * total_length)
    train_list_text = text_data[:train_length]
    test_list_text = text_data[train_length:]
    train_list_label = author_data[:train_length]
    test_list_label = author_data[train_length:]
    train_data = {'text': train_list_text, 'label': train_list_label}
    test_data = {'text': test_list_text, 'label': test_list_label}
    return DatasetDict({'train': Dataset.from_dict(train_data), 'validation': Dataset.from_dict(test_data)})

def preprocess_function(examples):
    global tokenizer
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=512)

def custom_evaluate(trainer, eval_dataset):
    # Your custom_evaluate function adjusted for specific accuracy output
    id2label = {0: "austen", 1: "dickens", 2: "tolstoy", 3: "wilde"}
    raw_pred, labels, _ = trainer.predict(eval_dataset)
    predictions = np.argmax(raw_pred, axis=1)
    for label_id, label in id2label.items():
        label_indices = labels == label_id
        correct = predictions[label_indices] == labels[label_indices]
        accuracy = correct.mean()
        print(f"{label}\t{accuracy * 100:.2f}% correct")

def classify_sentences_from_file(file_path, model_path, tokenizer):
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    sentences = open(file_path, 'r', encoding='utf-8').readlines()
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        logits = model(**inputs).logits
    predictions = torch.argmax(logits, dim=1)
    for sentence, pred in zip(sentences, predictions):
        print(f"'{sentence}' is predicted to be written by {model.config.id2label[pred.item()]}.")



# Main function modified to dispatch based on the approach
def main():
    global tokenizer  # Reference the global tokenizer variable
    args = sys.argv[1:]  # Ignore the script file name

    if '-approach' not in args:
        print("Approach not specified correctly.")
        sys.exit(1)

    approach_index = args.index('-approach') + 1
    approach = args[approach_index] if approach_index < len(args) else None

    test_path = args[args.index('-test') + 1] if '-test' in args else None
    authorlist_path = args[0]

    if approach == 'generative':
        with open(authorlist_path, encoding="utf8") as authorlist_file:
            authorlist = authorlist_file.read()
            if test_path is None:
                dev_model = Dev(authorlist)
                dev_model.run()
            else:
                with open(test_path, encoding="utf8") as test_file:
                    test_content = test_file.read()
                    test_model = Test(authorlist, test_content)
                    test_model.run()
    elif approach == 'discriminative':
        model_name = 'distilbert-base-uncased'
        tokenizer = AutoTokenizer.from_pretrained(model_name)  # Define tokenizer here

        if not test_path:
            author_dataset = create_dataset({
                'austen_utf8.txt': 0,
                'dickens_utf8.txt': 1,
                'tolstoy_utf8.txt': 2,
                'wilde_utf8.txt': 3
            })
            tokenized_dataset = author_dataset.map(lambda examples: preprocess_function(examples), batched=True)

            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, id2label={0: "austen", 1: "dickens", 2: "tolstoy", 3: "wilde"}, label2id={"austen": 0, "dickens": 1, "tolstoy": 2, "wilde": 3})
            training_args = TrainingArguments(output_dir='./results', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, evaluation_strategy='epoch', save_strategy='epoch', load_best_model_at_end=True)
            trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset['train'], eval_dataset=tokenized_dataset['validation'], tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer=tokenizer))
            trainer.train()
            model.save_pretrained('./author_classification_model')
            tokenizer.save_pretrained('./author_classification_model')
            custom_evaluate(trainer, tokenized_dataset['validation'])
        else:
            model_path = './author_classification_model'
            classify_sentences_from_file(test_path, model_path, tokenizer)
    else:
        print("Invalid approach specified.")
        sys.exit(1)

if __name__ == '__main__':
    main()
