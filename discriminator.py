# -*- coding: utf-8 -*-
"""discriminator

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1StnfZry0GyIRWhF25mtf0uCt7fX3Ugkz
"""

import sys
import numpy as np
import pandas as pd
import torch
from nltk.tokenize import sent_tokenize
import nltk
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import Dataset, DatasetDict, load_metric

nltk.download('punkt')

# Define the file paths for your text files
file_path_auten = '/content/austen_utf8.txt'
file_path_dickens = '/content/dickens_utf8.txt'
file_path_tolstoy = '/content/tolstoy_utf8.txt'
file_path_wilde = '/content/wilde_utf8.txt'
test_file_path = ''  # This will be updated based on command line argument

# Author dictionary
author_dict = {file_path_auten: 0, file_path_dickens: 1, file_path_tolstoy: 2, file_path_wilde: 3}

def create_dataset(author_dict):
    full_list_text = []
    full_list_label = []
    for file_path, author in author_dict.items():
        with open(file_path, 'r') as file:
            lines = file.readlines()
            text_data = ''.join(lines)
        text_without_newlines = text_data.replace('\n', ' ')
        sentences = sent_tokenize(text_without_newlines)
        full_list_text += sentences
        full_list_label += [author for _ in range(len(sentences))]
    combined = list(zip(full_list_text, full_list_label))
    random.shuffle(combined)
    text_data, author_data = zip(*combined)
    total_length = len(text_data)
    train_length = int(0.8 * total_length)
    train_list_text = text_data[:train_length]
    test_list_text = text_data[train_length:]
    train_list_label = author_data[:train_length]
    test_list_label = author_data[train_length:]
    train_data = {'text': train_list_text, 'label': train_list_label}
    test_data = {'text': test_list_text, 'label': test_list_label}
    return DatasetDict({'train': Dataset.from_dict(train_data), 'validation': Dataset.from_dict(test_data)})

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=512)

def custom_evaluate(trainer, eval_dataset):
    raw_pred, labels, _ = trainer.predict(eval_dataset)
    predictions = np.argmax(raw_pred, axis=1)
    for author_id, author_name in id2label.items():
        author_mask = labels == author_id
        author_predictions = predictions[author_mask]
        author_labels = labels[author_mask]
        if len(author_labels) > 0:
            accuracy = np.mean(author_predictions == author_labels)
            print(f"{author_name}\t{accuracy * 100:.2f}% correct")
        else:
            print(f"{author_name}\tNo data for this author in the validation set.")

def classify_sentences_from_file(file_path, model_path, tokenizer):
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    sentences = open(file_path, 'r').readlines()
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        logits = model(**inputs).logits
    predictions = torch.argmax(logits, dim=1)
    for sentence, pred in zip(sentences, predictions):
        print(f"'{sentence}' is predicted to be written by {id2label[pred.item()]}.")

# Parse command-line arguments
if __name__ == "__main__":
    approach = None
    test_file_path = None

    if '-approach' in sys.argv:
        approach_index = sys.argv.index('-approach') + 1
        approach = sys.argv[approach_index]

    if '-test' in sys.argv:
        test_index = sys.argv.index('-test') + 1
        test_file_path = sys.argv[test_index]

    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    id2label = {0: "austen", 1: "dickens", 2: "tolstoy", 3: "wilde"}
    label2id = {"austen": 0, "dickens": 1, "tolstoy": 2, "wilde": 3}
    model_name = 'distilbert-base-uncased'

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(author_dict), id2label=id2label, label2id=label2id)
    training_args = TrainingArguments(output_dir='/content/Ngram_classifier', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, evaluation_strategy='epoch', save_strategy='epoch', load_best_model_at_end=True)

    if approach == "discriminative" and not test_file_path:
        # Train and evaluate model accuracy
        author_dataset = create_dataset(author_dict)
        tokenized_dataset = author_dataset.map(preprocess_function, batched=True)
        trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset['train'], eval_dataset=tokenized_dataset['validation'], tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer=tokenizer))
        trainer.train()
        model.save_pretrained('/content/author_classification_model')
        tokenizer.save_pretrained('/content/author_classification_model')
        custom_evaluate(trainer, tokenized_dataset['validation'])
    elif approach == "discriminative" and test_file_path:
        # Classify sentences from test file
        classify_sentences_from_file(test_file_path, '/content/author_classification_model', tokenizer)